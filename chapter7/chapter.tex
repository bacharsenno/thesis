% !TEX root = ../thesis.tex
\chapter{Implementation and Evaluation}
\label{capitolo7}
\thispagestyle{empty}
\section{Implementation}
\subsection{API Wrappers}
Building upon the generic wrapper concept, we decided to include the API-specific details of each web service in a JSON file associated with it. This means that all of the details concerning this service, such as the request URL, the authentication method and key, the expected URL parameters, and the format and fields of the response would be included in this JSON file. In a way, these JSON files are analogous with the XML descriptors that PerLa made use of to define the details of different web services. This proved to be a good approach because it allowed the actual code to be devoid of any API-specific code. This is an example of the details associated with the Yelp web service:
\begin{lstlisting}[language=json,firstnumber=1]
{
  "MainRequest": {
    "key": "Authorization: Bearer *Authentication Key*",
    "url": "https://api.yelp.com/v3/businesses/search?",
    "query": "term=",
    "city": "&location=",
    "lon": "&longitude=",
    "lat": "&latitude="
  },
  "ResponseParams": {
    "arrayLocation": "businesses",
    "name": "name",
    "address": "location.address1",
    "url": "url",
    "latitude": "coordinates.latitude",
    "longitude": "coordinates.longitude",
    "cuisines": "categories.0.title",
    "thumb": "image_url",
    "phone": "display_phone",
    "rating": "rating"
  }
}
\end{lstlisting}
\subsubsection{Mandatory Objects}
The JSON files include two mandatory objects, and three optional ones. The first mandatory object is the ``MainRequest'' object. This object contains details needed to know how to send the main request to the web service, such as the authentication key, the URL, and the query parameters. In case the authentication is done through a URL parameter, it is not included as a separate attribute; instead, it is directly added to the corresponding URL. The presence of a ``key'' attribute means that the authentication details are included in the header of the request (as opposed to URL authentication) and the value of this attribute is the header that must be attached before sending the request.\\\\
The second mandatory object is \emph{ResponseParams}, and its job is to define the structure of the response, as well as where to find the values we want to extract. In normal cases, the arrayLocation attribute points to the location of the object array so that we can extract; this object array in the case of Eventbrite for example is the actual array containing the individual events returned by the web service. If the object array is nested within other objects, the path to the array is encoded as objectName1::objectName2(...), and the code handles traversing the response based on the provided arrayLocation attribute until it reaches the desired location in the response.\\\\
In order to facilitate parsing the responses, particularly when it comes to multi-level nesting, we opted to use a flattening function, which flattens a multi-level array into a single level one. Moreover, the flattening function is implemented in a way that prevents loss of information, and that is guaranteed since the keys of the objects existing at deeper levels are simply concatenated to those of the parents objects, separated by a token. This means that even though we are flattening the array, we are still preserving the information concerning the keys and values of nested objects.\\\\
Although the responses from various APIs share some similarity in the general structure, there are some that are quite different, and these differences have to be encoded and included in the corresponding JSON descriptors. For instance, Zomato's response is peculiar because the object array does not contain simple restaurant objects; instead, each restaurant is encapsulated within an additional JSON object. Therefore, we need to loop over the array and extract the restaurants from these objects. This is specified in the JSON descriptor as follows:
\begin{lstlisting}[language=json,firstnumber=1]
{
  "PreRequest": {
    "key": "user-key: *Authentication Key*",
    "url": "https://developers.zomato.com/api/v2.1/cities?q=",
    "result": "location_suggestions.0.id"
  },
  "MainRequest": {
    "key": "user-key: *Authentication Key*",
    "url": "https://developers.zomato.com/api/v2.1/search?",
    "query": "q=",
    "city": "&entity_type=city&entity_id=",
    "lon": "&lon=",
    "lat": "&lat="
  },
  "ResponseParams": {
    "loop": true,
    "preloop": "restaurants",
    "postloop": "restaurant",
    "name": "name",
    "address": "location.address",
    "url": "url",
    "latitude": "location.latitude",
    "longitude": "location.longitude",
    "cuisines": "cuisines",
    "thumb": "featured_image",
    "phone": "phone_numbers",
    "rating": "user_rating.aggregate_rating"
  }
}
\end{lstlisting}
The loop attribute specifies that a loop is required to extract the restaurant instances from the response, the preloop attribute corresponds to the name of the outer structure that we need to loop over, and the postloop attribute represents the name of the inner object that contains the actual restaurant we want to extract.
\subsubsection{Optional Objects}
Next comes the optional part of our JSON descriptors: ``PreRequest'' and ``PostRequest''. The first object is responsible for any preliminary work that needs to be done before sending the requests to the web services, while the latter serves to perform all of the required post-processing on the received replies. Both of these objects are similar to some extent to the ``MainRequest'' object, but their purpose differs. These details are needed because some services are not as intuitive as others are; in other terms, sometimes, more work is required in order to get the desired response from an API, as opposed to simply sending a single request.\\\\
For example, the Zomato search API is designed in a certain way that does not accept city names as search parameters. Instead, the search request should include a CityID parameter if the aim is to do a city-based search. In order to perform the mapping between city names and city IDs, a request should be sent to the City API before sending the main request. The reply of this preliminary request contains the city ID that corresponds to the supplied city name. This ID will be embedded in the query that we will send later on using the ``MainRequest'' object.\\\\
Similarly, the Eventbrite API requires additional work, however this time the work must be done after receiving the response to the main request. In particular, Eventbrite returns a list of events, and each event object contains a venue ID linking to the venue where the event will take place. Since we are interested in getting these details about the venues, we are required to send an additional request, per event, to the Venues API. This request will have the Venue ID as parameter, and the response will contain details on the venue, such as the venue's name, address, and URL. This process is defined in the`` PostRequest'' object as shown below:
\begin{lstlisting}[language=json,firstnumber=1]
{
  "MainRequest": {
    "key": "Authorization: Bearer *Authentication Key*",
    "url": "https://www.eventbriteapi.com/v3/events/search?",
    "query": "q=",
    "city": "&location.address=",
    "lat": "&location.within=5km&location.latitude=",
    "lon": "&location.longitude="
  },
  "PostRequest": {
    "key": "Authorization: Bearer 26MMNKPXGH5HEISEKXPY",
    "scope": "item",
    "url": "https://www.eventbriteapi.com/v3/venues/",
    "param": "venue_id"
  },
  "ResponseParams": {
    "arrayLocation": "events",
    "title": "name.text",
    "venueName": "name",
    "venueAddress": "address.localized_address_display",
    "venueURL": "resource_uri",
    "eventURL": "url",
    "allDay": null,
    "startTime": "start.local",
    "endTime": "end.local",
    "longitude": "longitude",
    "latitude": "latitude"
  },
  "CustomObject": {
    "constructor": "$eventObject = new Event(isset($v['title'], $event[$v['title']]) ? $event[$v['title']] : '', isset($v['venueName'], $postObject[$v['venueName']]) ? $postObject[$v['venueName']] : '', isset($v['venueAddress'], $postObject[$v['venueAddress']]) ? $postObject[$v['venueAddress']] : '', isset($v['venueURL'], $postObject[$v['venueURL']]) ? $postObject[$v['venueURL']] : '', isset($v['eventURL'], $event[$v['eventURL']]) ? $event[$v['eventURL']] : '', isset($v['allDay'], $event[$v['allDay']]) ? $event[$v['allDay']] : '', isset($v['startTime'], $event[$v['startTime']]) ? $event[$v['startTime']] : '', isset($v['endTime'], $event[$v['endTime']]) ? $event[$v['endTime']] : '', isset($v['latitude'], $postObject[$v['latitude']]) ? $postObject[$v['latitude']] : '', isset($v['longitude'], $postObject[$v['longitude']]) ? $postObject[$v['longitude']] : '');"
  }
}
\end{lstlisting}
This object is almost identical to the PreRequest object, with only one additional parameter ``scope''. This parameter specifies whether the post-processing should happen on the response as a whole, or on an item-by-item basis. In the case of Eventbrite, since we need to get the venue details for each individual event, the scope is set to ``item''.\\\\
The fifth and last object that can be included in the JSON descriptors is the ``CustomObject'' object. This object serves to provide a way to define a custom constructor when needed. Since we have information coming from two different sources (the Event object and the Venue object), we had to define a custom constructor for the Event object, as opposed to the generic one defined in the main code file. The custom constructor call defined in the CustomObject object is parsed using the PHP Eval method, which interprets a string as actual PHP code and executes it. This allowed us to circumvent the need to include two different constructors in the code, one of the being written specifically for Eventbrite. This approach allowed us to stick to the generic wrapper philosophy we are aiming for.
\newpage
\subsection{Storage and Caching}
\subsubsection{Caching Mechanisms}
Various methods exist when it comes to caching, and one of the most intuitive caching techniques simply consists of saving the query history and the received responses in a SQL database. Databases fall under the relational category, and offer functionalities that go beyond simply storing data: they allow performing more complex operations such as filtering the data we're selecting, joining different tables in a database for a more comprehensive data selection, as well as some more advanced features such as creating triggers, views, and setting up live replication and recovery. Moreover, SQL databases are highly scalable, and can store virtually unlimited amounts of data since they rely on disk-based storage.\\\\
Another valid option would be to use in-memory caching, a technology offered by tools such as Memcached, or the newer and improved counterpart Redis. These caching technologies follow a different paradigm, as they do not offer relational functionalities; instead, both Memcached and Redis are essentially key-value stores that rely on storing information in the RAM.\\\\
This is the most fundamental difference between the two alternatives: unlike disk-based storage, memory-based storage does not offer as much storage space as the former, since RAM capacity is much more limited than disk capacity. On the other hand, disks cannot offer the same read/write speed offered by RAM, and that is why Redis is blazingly fast: the I/O operations are non-blocking, and Redis can perform over 100,000 read/writes per second.\\\\
It is obvious that we have a tradeoff here between capacity and speed: SQL makes more sense in terms of capacity, and Redis is the more logical option if speed is our primary concern and if we are dealing with somewhat limited amounts of data. In our case, we mentioned earlier that we have two different structures that we want to cache: we want to cache the queries issued, namely the category a query corresponds to, the location a query is issued from, the specific text linked to the query, and finally a timestamp representing when this query was issued. Using an in-memory cache for the queries turns out to be a good approach: the speed-space compromise works in our favor, the memory size is more than enough to store details about the queries, and we do not need any of the additional functionalities offered by relational databases.\\\\
This does not apply to storing the responses received from web services since scalability in this case is a much more prevalent issue: one simple query can result in a response containing tens, or even hundreds of elements. As the number of queries increases, the amounts of data we have to save will also increase, and a Redis approach will not be able to keep up with the space requirements.\\\\
Moreover, we want to preserve the structure of the different objects when saving in the database, in a way that matches the resource schema. This cannot be done using a key-value database unless if we find a way to encode the primary key of each object as the key of the key-value pair, and encode everything else, from the object-specific details to the various timestamps, as the value of the key-value pair. Performing such operations will add significant overhead, and this will become even more problematic when we need to decode this data. Finally, using a key-value cache to store objects will cause us to lose any possible join functionalities, which can be very beneficial when performing some more complex operations. Therefore, an SQL database seems to be the better option in this case.\\\\
The following diagram shows the caching schema, including details on which kind of cache is used for each component.
\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{cachedetails}
\caption{Caching Details}
\end{figure}
\newpage
\newpage
\subsubsection{How to cache a query?}
As we mentioned earlier, a query is identified by the following characteristics:
\begin{itemize}
  \item Category of the requested option (corresponding to an object type in the resource schema).
  \item Location of a query, originally received from the middleware as (latitude, longitude) pair.
  \item Query text, as in the user-provided content of the query.
  \item Timestamp of when the query was initially sent.
\end{itemize}
The first three characteristics allow us to identify whether two queries are, in a sense, ``identical''. Two queries are ``identical'' if they belong to the same category, have been issued from the same location (within a certain radius, and we take 5 kilometers as an example), and have the same text (or a close synonym). Conceptually, matching two queries is simple; programmatically however, we have to find a way to determine whether two location points are within a certain distance from one another on the fly, and whether two terms can be considered close synonyms.\\\\
For the first part of the key, e.g. the category, it is easy to distinguish whether two queries share the same category since this information is already provided by the middleware, and we simply have to include it as part of the key in the key-value pair.\\\\
Concerning the location, one approach would be to match a query with an area rather than a specific point: if we could identify whether two queries were issued from the same area. Geohash can help us do exactly that with minimum effort.
\subsubsection{What is Geohash?}
Geohash is a convenient way that allows encoding any given location, represented as a (latitude, longitude) pair, as an alphanumeric string. The longer the encoded string is, the more accurately a location is encoded. The following table represents the radiance of an area represented by a Geohash string, with respect to the length of this string:\\
\#  \hspace{5pt} km\\      
1   $\pm$ 2500\\
2   $\pm$ 630\\
3   $\pm$ 78\\
4   $\pm$ 20\\
5   $\pm$ 2.4\\
6   $\pm$ 0.61\\
7   $\pm$ 0.076\\
8   $\pm$ 0.019\\
9   $\pm$ 0.0024\\
10  $\pm$ 0.00060\\
11  $\pm$ 0.000074\\
\\
This means that if two Geohash strings start with the same character, then the two corresponding locations are at most 2500 kilometers away from one another. Applying the same idea to our case, and taking two query locations as an example, if the corresponding Geohash strings have the same 5-character prefix, then the distance between the two locations of queries is less than or equal to 2.4 KM. In other words, a 5-character prefix corresponds to a circle of diameter 4.8 kilometers, and all queries that share the same 5-character prefix belong in the same circle. This makes our task much easier, since we now have a reliable way to distinguish whether two queries match in terms of location, by performing simple calculations.\\\\
So far, we have successfully encoded two of the three key characteristics as part of the key, and the key is structured as follows:\\
Category:::Geohash\\
Example:\\
Restaurants:::drt2z\\\\
The next step would be to add the query text to the key. However, if we base our key checks on literal comparisons of various query texts, we would not be establishing a real equivalency relation. For instance, if a user performs a search for ``music'' events in an area, and another user shortly after that searches for ``band'', there will be many common elements in the results of these two operations. Similarly, if a user searches for ``Asian'' restaurants, and another searches for ``Chinese'' restaurants in the same location, many restaurants will exist in both result lists. Therefore, we should establish some contextual equivalency measure between these terms.
\pagebreak
\subsubsection{Query Text Equivalency}
We want to be able to link two queries by comparing the text attached to them, from a semantic perspective, and decide whether they can be considered equivalent. Multiple approaches exist that allow us to compare semantic resemblance between terms.
\begin{itemize}
\item Word2vec\\
Word2vec\footnote{https://code.google.com/archive/p/word2vec/} is a tool developed by a team of researchers at Google in 2013. This tool uses shallow, two-layer neural networks as models, and these models are trained over large data sets in an attempt to find semantic and linguistic relations between words. Word2vec produces a vector space that represents the data set, each words being a vector in this space. The distance between two vectors is proportional to the semantic similarity between the two words associated with these vectors.\\\\
The code below calculates and prints the following information:
\begin{itemize}
\item Similarity between the words ``first'' and ``second''.
\item Similarity between the words ``sushi'' and ``japanese''.
\item The ten most similar words to ``sushi''.
\end{itemize}
The data set used is made up of the top English language pages on Wikipedia, and its size is around 1 Gigabyte.\\
\begin{lstlisting}[language=Python]
import gensim, logging
import gensim.downloader as api

info = api.info()
# download the model and return as object ready for use
model = api.load("glove-wiki-gigaword-300")  

print model.similarity("first", "second")
print model.similarity("sushi", "japanese")
print model.similarity("sushi", "music")
print model.wv.most_similar(positive="sushi")
\end{lstlisting}
\pagebreak
The output of this program is as follows:
\begin{itemize}
\item Similarity between ``first'' and ``second'': 0.8507271
\item Similarity between ``sushi'' and ``japanese'': 0.2515804
\item Similarity between ``sushi'' and ``music'': 0.07649763
\item Ten most similar words to ``sushi'':
\begin{itemize}
\item sashimi
\item restaurant
\item restaurants
\item chefs
\item diners
\item seafood
\item chef
\item steak
\item sandwich
\item tempura
\end{itemize}
\end{itemize}
Looking at the output, we notice that there is a degree of accuracy in terms of semantic similarity; the similarity coefficient between ``first'' and ``second'' is 85\%, while it is 25\% for ``sushi'' and ``japanese'', and 7\% for ``sushi'' and ``music''. At first glance, the output makes sense. However, for our purposes, Word2vec might not always offer the kind of semantic analysis that we are going for. For instance, Word2vec gives a high similarity coefficient between ``sushi'' and ``burger'', since both of them are types of food. However, in our context, we want a very low similarity coefficient between these two terms. More investigation resulted in many more examples of cases where Word2vec alone actually leads us the wrong way. Therefore, even though it can provide valuable information in some contexts, we cannot solely rely on it. We had to look for other approaches.
\pagebreak
\item Database Query Text Associations\\
As mentioned earlier, we are storing in an SQL database the content of the replies we are receiving from the various web services, after parsing them and matching the needed attributes with the resource schema objects. However, we are also saving information about the queries that led us to these results. An example of this is shown in the figure below:
\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{dbquery}
\caption{Resource Database Sample}
\end{figure}
\\
For each received object (restaurants, in this case), we save the source (web service) that sent it, as well as the location and the text of the query associated with it. For example, the first result in the table is a restaurant called ``Crudo'', and there are three different queries that returned this same restaurant as part of the result set: ``sushi'', ``asian'', and ``japanese''. The third restaurant ``Koy'' is associated with queries having ``sushi'', ``asian'', and ``korean'' as query text.\\\\
The interesting point here is that since some query texts return the same restaurant, then there is a chance that the associated terms have some kind of correlation among them. This correlation might be the semantic similarity we are looking for. Therefore, once the number of users scales enough and we have more data, we might be able to mine for valid semantic associations between various terms.
\pagebreak
\item Manual Ontology Definition\\
The third approach to determine whether two queries are equivalent by relying on static rules that are defined and saved in the system. The system administrator, along with someone who is experienced in the relevant domain, can define these rules and save them. While this might seem unfeasible due to the number of variations a word can have, we only need to save the root of the words manually, and link them with other semantically related roots.\\\\
For example, in the case of restaurants, the administrator would define through rules that ``asian'', ``japanese'', and ``sushi'' are linked. Likewise, there would be another rule linking ``pizza'', ``pasta'', and ``italian''. After doing this, we can generate programmatically some variations of these words using string manipulation and regular expressions. In other words, terms like ``japan'', ``japanese food'', and ``sushi bar'' are generated automatically once the corresponding roots have been added to our semantic dictionary.\\\\
One last point worth mentioning in the following: we are aiming to develop CAMUS as a framework that can be adopted and implement in as many fields as possible. The tourism scenario we have been using as an example is just that: an example. The system however is being developed in a generic way that allows it to be beneficial in other domains as well. However, some of these domains might be much more sensitive and critical than tourism; some examples of that would be anything related to medicine or security. In those cases, any kind of association rules would anyways need to be looked over by a specialist from the field, and this even applies to structuring the corresponding CDT and resource schema. Therefore, the idea of manually setting up some rules, even if it is just the most basic and important ones, now becomes more logical.
\end{itemize}
\pagebreak
\section{Evaluation}
WiP