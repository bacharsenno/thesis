% !TEX root = ../thesis.tex
\chapter{Implementation and Evaluation}
\label{capitolo7}
\thispagestyle{empty}
\section{Implementation}
\subsection{API Wrappers}
Building upon the generic wrapper concept, we decided to include the API-specific details of each web service in a JSON file associated with it. This means that all of the details concerning this service, such as the request URL, the authentication method and key, the expected URL parameters, and the format and fields of the response would be included in this JSON file. In a way, these JSON files are analogous with the XML descriptors that PerLa made use of to define the details of different web services. This proved to be a good approach because it allowed the actual code to be devoid of any API-specific code. This is an example of the details associated with the Yelp web service:
\begin{lstlisting}[language=json,firstnumber=1]
{
  "MainRequest": {
    "key": "Authorization: Bearer *Authentication Key*",
    "url": "https://api.yelp.com/v3/businesses/search?",
    "query": "term=",
    "city": "&location=",
    "lon": "&longitude=",
    "lat": "&latitude="
  },
  "ResponseParams": {
    "arrayLocation": "businesses",
    "name": "name",
    "address": "location.address1",
    "url": "url",
    "latitude": "coordinates.latitude",
    "longitude": "coordinates.longitude",
    "cuisines": "categories.0.title",
    "thumb": "image_url",
    "phone": "display_phone",
    "rating": "rating"
  }
}
\end{lstlisting}
\subsubsection{Mandatory Objects}
The JSON files include two mandatory objects, and three optional ones.\\\\
\textbf{\textit{MainRequest:}} The first mandatory object is the \emph{MainRequest} object. This object contains details needed to know how to send the main request to the web service, such as the authentication key, the URL, and the query parameters. In case the authentication is done through a URL parameter, it is not included as a separate attribute; instead, it is directly added to the corresponding URL. The presence of a ``key'' attribute means that the authentication details are included in the header of the request (as opposed to URL authentication) and the value of this attribute is the header that must be attached before sending the request.\\\\
\textbf{\textit{ResponseParams:}} The second mandatory object is \emph{ResponseParams}, and its job is to define the structure of the response, as well as where to find the values we want to extract. In normal cases, the ``arrayLocation'' attribute points to the location of the object array so that we can extract; this object array in the case of Eventbrite for example is the actual array containing the individual events returned by the web service. If the object array is nested within other objects, the path to the array is encoded as objectName1:::objectName2(...), and the code handles traversing the response based on the provided arrayLocation attribute until it reaches the desired location in the response.\\\\
In order to facilitate parsing the responses, particularly when it comes to multi-level nesting, we opted to use a flattening function, which flattens a multi-level array into a single level one. Moreover, the flattening function is implemented in a way that prevents loss of information, and that is guaranteed since the keys of the objects existing at deeper levels are simply concatenated to those of the parents objects, separated by a token. This means that even though we are flattening the array, we are still preserving the information concerning the keys and values of nested objects, as well as their nesting level.\\\\
Although the responses from various APIs share some similarity in the general structure, there are some that are quite different, and these differences have to be encoded and included in the corresponding JSON descriptors. For instance, Zomato's response is peculiar because the object array does not contain simple restaurant objects; instead, each restaurant is encapsulated within an additional JSON object. Therefore, we need to loop over the array and extract the restaurants from these objects. This is specified in the JSON descriptor as follows:
\begin{lstlisting}[language=json,firstnumber=1]
{
  "PreRequest": {
    "key": "user-key: *Authentication Key*",
    "url": "https://developers.zomato.com/api/v2.1/cities?q=",
    "result": "location_suggestions.0.id"
  },
  "MainRequest": {
    "key": "user-key: *Authentication Key*",
    "url": "https://developers.zomato.com/api/v2.1/search?",
    "query": "q=",
    "city": "&entity_type=city&entity_id=",
    "lon": "&lon=",
    "lat": "&lat="
  },
  "ResponseParams": {
    "loop": true,
    "preloop": "restaurants",
    "postloop": "restaurant",
    "name": "name",
    "address": "location.address",
    "url": "url",
    "latitude": "location.latitude",
    "longitude": "location.longitude",
    "cuisines": "cuisines",
    "thumb": "featured_image",
    "phone": "phone_numbers",
    "rating": "user_rating.aggregate_rating"
  }
}
\end{lstlisting}
The ``loop'' attribute specifies that a loop is required to extract the restaurant instances from the response, the ``preloop'' attribute corresponds to the name of the outer structure that we need to loop over, and the ``postloop'' attribute represents the name of the inner object that contains the actual restaurant we want to extract.
\subsubsection{Optional Objects}
\textbf{\textit{PreRequest}} and \textbf{\textit{PostRequest:}} Next comes the optional part of our JSON descriptors: \emph{PreRequest} and \emph{PostRequest}. The first object is responsible for any preliminary work that needs to be done before sending the requests to the web services, while the latter serves to perform all of the required post-processing on the received replies. Both of these objects are similar to some extent to the \emph{MainRequest} object, but their purpose differs. These details are needed because some services are not as intuitive as others; in other terms, sometimes, more work is required in order to get the desired response from an API, as opposed to simply sending a single request.\\\\
For example, the Zomato search API is designed in such a way that it does not accept city names as search parameters. Instead, the search request should include a CityID parameter if the aim is to do a city-based search. In order to perform the mapping between city names and city IDs, a request should be sent to the City API before sending the main request. The reply of this preliminary request contains the city ID that corresponds to the supplied city name. This ID will be embedded in the query that we will send later on using the \emph{MainRequest} object.\\\\
Similarly, the Eventbrite API requires additional work that must be done after receiving the response to the main request: Eventbrite returns a list of events, and each event object contains a venue ID linking to the venue where the event will take place. Since we are interested in providing the user with details about the venues, we are required to send an additional request, per event, to the Venues API. This request will have the Venue ID as parameter, and the response will contain details on the venue, such as the venue's name, address, and URL. This process is defined in the \emph{PostRequest} object as shown below:
\begin{lstlisting}[language=json,firstnumber=1]
{
  "MainRequest": {
    "key": "Authorization: Bearer *Authentication Key*",
    "url": "https://www.eventbriteapi.com/v3/events/search?",
    "query": "q=",
    "city": "&location.address=",
    "lat": "&location.within=5km&location.latitude=",
    "lon": "&location.longitude="
  },
  "PostRequest": {
    "key": "Authorization: Bearer 26MMNKPXGH5HEISEKXPY",
    "scope": "item",
    "url": "https://www.eventbriteapi.com/v3/venues/",
    "param": "venue_id"
  },
  "ResponseParams": {
    "arrayLocation": "events",
    "title": "name.text",
    "venueName": "name",
    "venueAddress": "address.localized_address_display",
    "venueURL": "resource_uri",
    "eventURL": "url",
    "allDay": null,
    "startTime": "start.local",
    "endTime": "end.local",
    "longitude": "longitude",
    "latitude": "latitude"
  },
  "CustomObject": {
    "constructor": "$eventObject = new Event(isset($v['title'], $event[$v['title']]) ? $event[$v['title']] : '', isset($v['venueName'], $postObject[$v['venueName']]) ? $postObject[$v['venueName']] : '', isset($v['venueAddress'], $postObject[$v['venueAddress']]) ? $postObject[$v['venueAddress']] : '', isset($v['venueURL'], $postObject[$v['venueURL']]) ? $postObject[$v['venueURL']] : '', isset($v['eventURL'], $event[$v['eventURL']]) ? $event[$v['eventURL']] : '', isset($v['allDay'], $event[$v['allDay']]) ? $event[$v['allDay']] : '', isset($v['startTime'], $event[$v['startTime']]) ? $event[$v['startTime']] : '', isset($v['endTime'], $event[$v['endTime']]) ? $event[$v['endTime']] : '', isset($v['latitude'], $postObject[$v['latitude']]) ? $postObject[$v['latitude']] : '', isset($v['longitude'], $postObject[$v['longitude']]) ? $postObject[$v['longitude']] : '');"
  }
}
\end{lstlisting}
This object is almost identical to the PreRequest object, with only one additional parameter ``scope''. This parameter specifies whether the post-processing should happen on the response as a whole, or on an item-by-item basis. In the case of Eventbrite, since we need to get the venue details for each individual event, the scope is set to ``item''.\\\\
\textbf{\textit{CustomObject:}} The fifth and final object that can be included in the JSON descriptors is the \emph{CustomObject} object. This object serves to provide a way to define a custom constructor when needed. Since we have information coming from two different sources (the Event object and the Venue object), we had to define a custom constructor for the Event object, as opposed to the generic one defined in the main code file. The custom constructor call defined in the \emph{CustomObject} object is parsed using the PHP Eval method, which interprets a string as actual PHP code and executes it. This allowed us to circumvent the need to include two different constructors in the code (a generic one, and a second one specifically for Eventbrite). This object can be abstracted as an optional object that can include any service-specific code that we might need to execute in some special cases, therefore preserving the generality of the main wrapper code.
\newpage
\subsection{Storage and Caching}
\subsubsection{Caching Mechanisms}
A cache, in its simplest form, is a hardware or software component that can store data, allowing future requests to be served faster. Since the data is already saved, the system wouldn't have to repeat the same process multiple times (an example of such processes would be performing complex calculations). Instead, it would simply return the result which is already stored in the cache.\\\\
In software, various methods exist when it comes to caching:\\\\
\textbf{\textit{RDBMS:}} One of the most intuitive caching techniques simply consists of saving the query history and the received responses in a relational database. Relational database management systems (RDBMS) offer functionalities that go beyond simply storing data: they allow performing more complex operations such as filtering the data we're selecting, joining different tables in a database for a more comprehensive data selection, as well as some more advanced features such as creating triggers, views, and setting up live replication and recovery. Moreover, such databases are highly scalable, and can store virtually unlimited amounts of data since they rely on disk-based storage.\\\\
\textbf{\textit{In-Memory Cache:}} Another valid option would be to use in-memory caching, a technology offered by tools such as Memcached, or the newer and improved counterpart Redis. These caching technologies follow a different paradigm, as they do not offer relational functionalities; instead, both Memcached and Redis are essentially key-value stores that rely on storing information in the RAM.\\\\
This is the most fundamental difference between the two alternatives: unlike disk-based storage, memory-based storage does not offer as much storage space as the former, since RAM capacity is much more limited than disk capacity. On the other hand, disks cannot offer the same read/write speed offered by RAM, and that is why Redis is blazingly fast: the I/O operations are non-blocking, and Redis can perform over 100,000 read/writes per second\footnote{https://static.simonwillison.net/static/2010/redis-tutorial/}.\\\\
It is obvious that we have a tradeoff here between capacity and speed: a RDBMS makes more sense in terms of capacity, and Redis is the more logical option if speed is our primary concern and if we are dealing with somewhat limited amounts of data. In our case, we mentioned earlier that we have two different structures that we want to cache: we want to cache the queries, namely the category a query corresponds to, the location a query is issued from, the specific text linked to the query, and finally a timestamp representing when this query was issued. Using an in-memory cache for the queries turns out to be a good approach: the speed-space compromise works in our favor, the memory size is more than enough to store details about the queries, and we do not need any of the additional functionalities offered by relational databases.\\\\
This does not apply to storing the responses received from web services, since scalability in this case is a much more prevalent issue: one simple query can result in a response containing tens, or even hundreds of elements. As the number of queries increases, the amounts of data we have to save will also increase, and a Redis approach will not be able to keep up with the space requirements.\\\\
Moreover, we want to preserve the structure of the different objects when saving in the database, in a way that matches the resource schema. This cannot be done using a key-value database unless we find a way to encode the primary key of each object as the key of the key-value pair, and encode everything else, from the object-specific details to the various timestamps, as the value of the key-value pair. Performing such operations will add significant overhead, and this will become even more problematic when we need to decode this data. Finally, using a key-value cache to store objects will cause us to lose the join functionalities, which can be very beneficial when performing some complex operations. Therefore, a RDBMS seems to be the better option in this case.\\\\
The following diagram shows the caching schema, including details on which kind of cache is used for each component. It is identical to the diagram shown in figure 4.4, and follows the same flows, but it also includes the exact technologies we will use for each caching phase.
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{cachedetails}
\caption{Caching Details}
\end{figure}
\subsubsection{Query Caching}
As we mentioned earlier, a query is identified by the following characteristics:
\begin{itemize}
  \item Category of the requested option (corresponding to an object type in the resource schema).
  \item Location of a query, originally received from the middleware as (latitude, longitude) pair.
  \item Query text, as in the user-provided content of the query.
  \item Timestamp of when the query was initially sent.
\end{itemize}
The first three characteristics allow us to identify whether two queries are, in a sense, ``identical''. Two queries are ``identical'' if they belong to the same category, have been issued from the same location (within a certain radius, and we take 2.4 kilometers as an example), and have the same text (or a close synonym). Thus, we have to find a way to determine whether two location points are within a certain distance from one another on the fly, and whether two terms can be considered close synonyms.\\\\
For the first part of the key, e.g. the category, it is easy to distinguish whether two queries share the same category since this information is already provided by the middleware, and we simply have to include it as part of the key in the key-value pair.\\\\
In what follows, we try to explain the adopted strategies for establishing location equivalence and query text equivalence.
\subsubsection{Location Equivalence}
Geohash\footnote{http://geohash.org/} is a public domain geocoding system which encodes a geographic location -initially represented as a (latitude, longitude) pair- into a short string of letters and digits. The longer the encoded string is, the more accurately a location is encoded. The following table represents the radius of an area represented by a Geohash string, with respect to the length of this string:\\
\#  \hspace{5pt} km\\      
1   $\pm$ 2500\\
2   $\pm$ 630\\
3   $\pm$ 78\\
4   $\pm$ 20\\
5   $\pm$ 2.4\\
6   $\pm$ 0.61\\
7   $\pm$ 0.076\\
8   $\pm$ 0.019\\
9   $\pm$ 0.0024\\
10  $\pm$ 0.00060\\
11  $\pm$ 0.000074\\
\\
This means that if two Geohash strings start with the same character, then the two corresponding locations are at most 2500 kilometers away from one another. Applying the same idea to our case, and taking two query locations as an example, if the corresponding Geohash strings have the same 5-character prefix, then the distance between the two locations of queries is less than or equal to 2.4 KM. In other words, a 5-character prefix corresponds to a circle of diameter 4.8 kilometers, and all queries that share the same 5-character prefix belong in the same circle. This makes our task much easier, since we now have a reliable way to distinguish whether two queries match in terms of location, by performing simple calculations.
\subsubsection{Query Text Equivalence}
So far, we have successfully encoded two of the three key characteristics as part of the Redis key, and the key is structured as follows:\\
Category:::Geohash\\
Example:\\
Restaurants:::drt2z\\\\
The next step would be to add the query text to the key. However, if we base our key checks on literal comparisons of various query texts, we would not be establishing a real equivalency relation. For instance, if a user performs a search for ``music'' events in an area, and another user shortly after that searches for ``band'', there will be many common elements in the results of these two operations. Similarly, if a user searches for ``Asian'' restaurants, and another searches for ``Chinese'' restaurants in the same location, many restaurants will exist in both result lists. Therefore, we should establish some semantic equivalency measure between these terms.
We want to be able to link two queries by comparing the text attached to them, from a semantic perspective, and decide whether they can be considered equivalent. There are multiple approaches that allow us to compare semantic resemblance between terms, and we will discuss in what follows.
\newpage
\begin{itemize}
\item Word2vec\\
Word2vec\footnote{https://code.google.com/archive/p/word2vec/} is a tool developed by a team of researchers at Google in 2013. This tool uses shallow, two-layer neural networks as models, and these models are trained over large data sets in an attempt to find semantic and linguistic relations between words. Word2vec produces a vector space that represents the data set, each words being a vector in this space. The distance between two vectors is proportional to the semantic similarity between the two words associated with these vectors.\\\\
The code below calculates and prints the following information:
\begin{itemize}
\item Similarity between the words ``first'' and ``second''.
\item Similarity between the words ``sushi'' and ``japanese''.
\item The ten most similar words to ``sushi''.
\end{itemize}
The training data set that we used is made up of the top English language pages on Wikipedia, and its size is around 1 Gigabyte.\\
\begin{lstlisting}[language=Python]
import gensim, logging
import gensim.downloader as api

info = api.info()
# download the model and return as object ready for use
model = api.load("glove-wiki-gigaword-300")  

print model.similarity("first", "second")
print model.similarity("sushi", "japanese")
print model.similarity("sushi", "music")
print model.wv.most_similar(positive="sushi")
\end{lstlisting}
The output of this program is as follows:
\begin{itemize}
\item Similarity between ``first'' and ``second'': 0.8507271
\item Similarity between ``sushi'' and ``japanese'': 0.2515804
\item Similarity between ``sushi'' and ``music'': 0.07649763
\newpage
\item Ten most similar words to ``sushi'':
\begin{itemize}
\item sashimi
\item restaurant
\item restaurants
\item chefs
\item diners
\item seafood
\item chef
\item steak
\item sandwich
\item tempura
\end{itemize}
\end{itemize}
Looking at the output, we notice that there is a variable degree of accuracy in terms of semantic similarity; the similarity coefficient between ``first'' and ``second'' is 85\%, while it is 25\% for ``sushi'' and ``japanese'', and 7\% for ``sushi'' and ``music''. At first glance, the output might make sense. However, for our purposes, Word2vec might not always offer the kind of semantic analysis that we are going for. For instance, Word2vec gives a high similarity coefficient between ``sushi'' and ``burger'', since both of them are types of food. However, in our context, we want a very low similarity coefficient between these two terms.\\\\
More investigation resulted in many more examples of cases where Word2vec alone actually leads us the wrong way. An example of that would be the high similarity coefficient that Word2vec assigns between a term and its negation (i.e. ``sushi'' and ``not sushi''). In case Word2vec is used as a similarity measure, we have to make sure that negation is not allowed in a query.\\\\
For the aforementioned reasons, even though Word2vec can provide valuable information in some contexts, we cannot solely rely on it, and had to look for other approaches.
\newpage
\item Database Query Text Associations\\
As mentioned earlier, we are storing in a relational database the content of the replies we are receiving from the various web services, after parsing them and matching the needed attributes with the resource schema objects. However, we are also saving information about the queries that led us to these results. An example of this is shown in the figure below:
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{dbquery}
\caption{Sample of Query Metadata}
\end{figure}
\\
For each received object (restaurants, in this case), we save the source (web service) that sent it, as well as the location and the text of the query associated with it. For example, the first result in the table is a restaurant called ``Crudo'', and there are three different queries that returned this same restaurant as part of the result set: ``sushi'', ``asian'', and ``japanese''. The third restaurant ``Koy'' is associated with queries having ``sushi'', ``asian'', and ``korean'' as query text.\\\\
The interesting point here is that since some query texts return the same restaurant, then there is a chance that the associated terms have some kind of correlation among them. This correlation might be a way to define the semantic similarity we are looking for. Therefore, once the number of users scales enough and we have more data, we might be able to mine for valid semantic associations between various terms.
\pagebreak
\item Manual Ontology Definition\\
The third approach to determine whether two queries are equivalent is by relying on static rules that are defined and saved in the form of domain ontology. The system administrator, along with someone who is experienced in the relevant domain, can define these rules and save them. While this might seem unfeasible due to the number of variations a word can have, we only need to save the root of the words manually, and link them with other semantically related roots.\\\\
For example, in the case of restaurants, the administrator would define through rules that ``asian'', ``japanese'', and ``sushi'' are linked. Likewise, there would be another rule linking ``pizza'', ``pasta'', and ``italian''. After doing this, we can generate programmatically some variations of these words using string manipulation and regular expressions. In other words, terms like ``japan'', ``japanese food'', and ``sushi bar'' are generated automatically once the corresponding roots have been added to our semantic dictionary.\\\\
One last point worth mentioning is the following: we are aiming to develop CAMUS as a framework that can be adopted and implemented in as many fields as possible. The tourism scenario we have been using is just an example. The system however is being developed in a generic way that allows it to be beneficial in other domains as well. However, some of these domains might be much more sensitive and critical than tourism; some examples of that would be anything related to medicine or security. In those cases, any kind of association rules would anyway need to be looked over by a specialist from the field, and this even applies to structuring the corresponding CDT and resource schema. Therefore, the idea of manually setting up some rules, even if it is just the most basic and important ones, now becomes even more appropriate.
\end{itemize}
\pagebreak
\section{Evaluation}
The evaluation process on one hand aims to confirm that the developed components are able to perform the needed functionalities successfully without any errors. On the other hand, the software development process is iterative, and therefore, there is always room to optimize and improve the performance of the various parts that make up our framework.\\\\
In this performance evaluation process, we focus on the parts that we worked the most on, namely the back-end system, and its communication with the middleware. We study different aspects in our system, such as its ability to include new web services given valid descriptors and the time it takes to retrieve and integrate data. Moreover, we focus on judging how advantageous introducing caching and storage mechanisms has been, as well as whether the theoretical design decision we made are supported by the results we get by doing experimental tests.
\subsection{Adding New Services}
Testing the flexibility of our algorithms, and the ability to add new services yielded satisfying results: our initial configuration included two main categories (restaurants and events). We also were integrating data from four different web services when searching for events, and from three when searching for restaurants. Adding a fourth web service to the restaurants API was easy and intuitive as we aimed for it to be: by generating and including the corresponding descriptor, the response received from this web service was integrated seamlessly with our previous ones.\\\\
Adding a new category (news) proved to be simple as well: we used the same code we had for the previous categories with very minimal changes, defined the new endpoint, and provided descriptors for three web services that provide news headlines. The new endpoint returned the expected response: a list of news headlines that integrates data received from all three new web services.
\subsection{Caching and Storage Evaluation}
One of the main reasons we introduced caching and storage is to reduce the amount of time required to fetch data from web services and integrate the responses. This process proved to be very time consuming, and therefore we proposed this approach as a solution. In order to test the efficiency of our solution, we devised the following plan:
\begin{itemize}
\item Generate a test dataset made up of 1000 queries, issued from Milano. The queries follow a normal distribution in terms of location, having Milano Duomo as center, to keep the scenario as realistic as possible.
\item The dataset includes entries for restaurants with various search queries (``sushi'', ``burger'', ``pizza'' etc...).
\item Choose a caching policy, and then forward the queries to the backend. Log the total running time, the standard deviation, and the variance for each of the policies.
\end{itemize}
The following map visualizes the geographical distribution of the queries.
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{testmap}
\caption{Geographical Query Distribution}
\end{figure}
\newpage
Moreover, we ran the tests using four different caching policies:
\begin{itemize}
\item NO CACHE: This policy does not implement any kind of caching or storage. For every query, the middleware forwards the query to the backend through an endpoint, and the backend queries the corresponding web services and integrates their outputs.
\item DB RESOURCE CACHE: Each time the backend integrates the responses received from web services, it stores the result in an SQL database, and it specifies the metadata that led to this result for each object (location and text of the corresponding query). For subsequent requests, the middleware tries to retrieve results from this same database, and only proceeds to forward requests to the backend in case the results retrieved from the database are deemed insufficient. 
\item DB CACHE (RESOURCE + QUERIES): Same as DB RESOURCE CACHE, but the backend also stores information about the queries (category, location, text, and time). Instead of querying the resource tables (which will scale exponentially as the number of users and queries increases), it simply checks whether an equivalent query has been issued within a certain timeframe. In theory, since the number of rows in a queries table is much smaller than that of a resource table, this process should be much faster.
\item DB CACHE (RESOURCE) + QUERIES CACHE (REDIS): Same as the previous case, but save the queries in a REDIS cache instead of an SQL database. The key-value pairs follow the same patterns defined in section 5.1.2.
\end{itemize}
The following table summarizes the results of the tests for each caching policy.
\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{1}{|l|}{\diagbox{Caching Policy}{Value}} & Average & Standard Deviation & Variance & Hit Ratio \\ \hline
NO CACHE & 2.4911 & 2.5054 & 6.2772 & N/A \\ \hline
RESOURCE CACHE (SQL) & 0.3675 & 1.0087 & 1.0175 & 85.6\% \\ \hline
\begin{tabular}[c]{@{}c@{}}RESOURCE CACHE (SQL)\\ +\\ QUERIES CACHE (SQL)\end{tabular} & 0.7833 & 1.430 & 1.3065 & 72.8\% \\ \hline
\begin{tabular}[c]{@{}c@{}}RESOURCE CACHE (SQL)\\ +\\ QUERIES CACHE (REDIS)\end{tabular} & 0.4184 & 1.1545 & 1.333 & 83.6\% \\ \hline
\end{tabular}%
}
\caption{Performance Evaluation Results}
\end{table}
\newpage
\noindent Right away, we can see the advantage gained by introducing caching. The average time per query (measured over 1000 queries) went down from 2.5 seconds to around 0.5 seconds for any caching type. Moreover, in accordance with our theoretical estimations, using REDIS has proved to be more efficient than using SQL databases as a query cache: this is where the in-memory storage and the non-blocking operations come into play.\\\\
One interesting observation is that caching the resources only in an SQL database has had better values compared to using both query and resource caching, even if minimally. However, this is probably because our resource cache started from an empty state, and therefore, towards the end of the test, it did not scale to the size that would match that of a real life scenario. The idea here is that the resource cache increases in size at a pace that is much higher than that of the query cache, since there are multiple resources associated with each query. Therefore, it becomes increasingly costly to go through the entirety of the resource cache and check whether it contains enough entries corresponding to the query and category we are looking for.\\\\
To sum up, the results we obtained after running these tests were encouraging, and they confirmed that we are on the right track with our current design decisions and overall architecture.