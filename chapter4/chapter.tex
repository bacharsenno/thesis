% !TEX root = ../thesis.tex
\chapter{Context-Aware Data Mashups: Approach}
\label{capitolo4}
\thispagestyle{empty}

\section{Design Decisions}
\subsection{The Case of PerLa}
PerLa is a middleware technology developed in Politecnico di Milano. The main aim of this middleware at the time was to query pervasive systems that exist in a heterogeneous environment, and provide a homogenous abstraction of the different devices that make up this system. In CAMUS, part of the system is going to have query different web services, and provide and abstract homogenous interface that contains the results of these queries. Therefore, using PerLa as a tool to provide such abstraction was a reasonable option. The way PerLa works is as follows: the first step is to generate (or manually write) an XML descriptor of the web service we want to query.\\\\
Since different sensors (in the case of pervasive systems) or different APIs (in the case of web services) often have different structures for their replies, the descriptor's job is to allow PerLa to know what to expect. The descriptor specifies what the structure of the reply is, as well as some other factors, such as the protocol used for communication (i.e. http, https etc...), the request method (GET, POST etc...), and the destination URL of the request. The descriptor also includes details on which response elements are interesting to us, so that we can retrieve and query for them. Finally, we can include at the end of the XML file any post-processing we want to do, in terms of editing the content of the response, using the PerLa scripting language.\\\\
However, it is noteworthy to mention that PerLa takes a bit of setting up before we could use it for REST APIs, and as we prepared PerLa for the needed tasks, we started figuring out the missing components and performing the necessary adjustments. In particular, PerLa did not natively support including HTTP headers when sending requests. Therefore, we had to locate the area in the code where the requests were being built and sent, and then we added the required code to support adding headers to requests.

\begin{lstlisting}
getRequest = new HttpGet(request.getUri());
String head = request.getHeaders();
if (head != null && !head.equals("")) {
	String[] headers = head.split(";");
	for (int i = 0; i < headers.length; i++) {
		String[] header = headers[i].split(":");
		getRequest.addHeader(header[0], header[1]);
	}
}
\end{lstlisting}

This code allows adding headers to the XML descriptor of the web service as follows: in the requests object, we add an attribute ``headers'', and assign to it the header as key:value. With this code, we can also add multiple headers, and we separate the different key-value pairs using semicolons. We also edited other various areas in the code to make this code work, as building the HTTP request goes through multiple phases in different Java classes.\\\\
Preliminary results with PerLa seemed promising: we were able to adapt this stable, solid framework to our needs. We performed these tests manually by writing XML descriptors for simple services (mostly weather services) because the responses were simple enough, and mapping them to XML was not too challenging. Naturally, we also want our code to work with more complex responses, as this is the case for most modern APIs. Modern APIs return responses that include multiple objects of various structured, nested arrays, and such structures are not simple when it comes to mapping them to XML descriptors for PerLa.\\\\
For this reason, the next step we decided to work on is finding a way to generate programmatically XML descriptors for web services based on the JSON responses returned by these services. The descriptor generator should be able to parse the response, figure out the schema of this response regardless of the actual data context returned, and output an XML descriptor that we would directly plug into PerLa. If everything goes as planned, this descriptor should work with any response returned by a specific API endpoint, regardless of which query parameters we include when querying the service.\\\\
After doing some research, we found that there are many libraries written in various languages that can generate a JSON Schema-like structure for any JSON document. This was interesting because mapping a JSON schema to XML schema is much simpler than mapping a JSON response to an XML schema. Thus, by using such libraries already, we would already be half-way on the path of generating the XML schema that PerLa requires. We ended up using a Jskemator, a public Python library available on Github. This library did its job perfectly well, and the next step would be to add the code needed to map JSON schema to XML schema. We wrote the corresponding code in Python as well, and as we tried it on different web services, we started figuring out some particularities in the way PerLa parses the XML descriptors. Specifically, when generating the response schema, we should define the different objects in a bottom-up approach. This means that in the case of nested objects for example, we should define the innermost object first, and then progressively go outwards until we reach the top level. At first sight, this does not seem very complicated, and all we have to do is keep track of the current nesting level we have reached, and then re-order all of the structures we have generated based on the nesting order. After testing with more services, we realized that it is not exactly that easy to solve this issue for the reasons mentioned in the following point.\\\\
The second point we noticed is that PerLa in its current implementation does not support the presence of multiple objects in the JSON replies sharing the same ID. Therefore, even if the original response contains such objects, and in case they do not share the same structure, our code must somehow make the IDs in the corresponding XML schema distinct. However, this proved to be more difficult than it sounds when querying some of the more complex services we mentioned. Taking Foursquare as an example, the response contains several nested objects that share the same IDs as the parent objects (sometimes several levels going up), and this created the problem of circular dependency, as we ended up with the inner objects being dependent on outer ones.\\\\
The second issue we ran into was when realized that since the URL we are querying is hard-coded in the descriptor, we needed to create a different XML file not only for each service we query, but also for each query to the same service any time we change the value of a query parameter. This is a problem because in a real life scenario, we will be querying the services multiple times, with different parameters, and this means that PerLa is starting to seem more troublesome than useful by this point. Moreover, after querying different services, we will be performing joins in order to integrate the different responses into one logical group. PerLa does not natively support any join operations, and so we will have to add such features after performing the queries through PerLa.\\\\
Finally yet importantly, PerLa supports querying multiple services simultaneously. However, in case the services share objects that have the same name, PerLa does not support choosing which service to query for this object. This is yet another thing that we will have to add to the core code of PerLa. After doing these tests and realizing the shortcomings of PerLa, we opted not to use it, since the work we will need to do in order to add the needed features will be very time-consuming, and this outweighs the advantages of using a this well-established framework. In the end, we decided to perform simply the queries to the services manually using any programming language of our choice, therefore bypassing the need to upgrade PerLa to have it satisfy our requirements.
\newpage
\section{Architecture}
\label{sec:architecture}
After making sure that in its current state, PerLa does not satisfy our needs, we decided to move forward without including it as a component in the CAMUS system. Therefore, we now had complete freedom in choosing the language in which to implement the different components we need. We opted to use PHP to send the requests and parse the responses. Next, we had to find numerous free APIs belonging to a specific category, and these APIs must fit within the Travel Agent scenario. This was quite a difficult task because there aren't many public free APIs that provide a generic search functionality and go well with the proposed scenario. Fortunately, in the end we were able to find three APIs that allow searching for events in a specific location (PredictHQ, Ticketmaster, Eventful, Eventbrite), and three APIs that lists restaurants also given a location (Zomato, Yelp, Foursquare). This makes sense in terms of travelling activities because tourists are usually interested in both finding places to eat and events to attend. This also was interesting because it opens the possibility of performing joins between these two categories (i.e. searching for sushi restaurants that are located around a certain event).\\\\
After choosing these seven APIs, we wrote the code needed to query each of them, and proceeded to study the differences in the way each of them expects a request and provides a reply. Naturally, different APIs both require differently formed requests, and respond in variously structured formats. For instance, some APIs require header authentication, while others perform authentication using URL parameters.
\newpage
\begin{figure}[h]
\centering
\includegraphics[scale=0.435]{genarch}
\caption{High-Level Architecture View}
\end{figure}
\newpage
\subsection{Wrapper Approach}
The next step was to find a way to generalize the code we are coding so that it can be easily adapted to any additional APIs we decide to add later on. Therefore, we needed to create a generic wrapper that, when given certain information about the requests and responses associated with a specific API, is able to query this API, retrieve the response, parse it, and integrate it with those of other APIs. This process is illustrated below, once again taking restaurants APIs as example.\\
\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{wrapper}
\caption{Service Wrapper}
\end{figure}
\\
In order to do that, we defined the structure of the objects we are querying for (i.e. restaurants and events), we specified the fields that interest us, and we defined the corresponding classes along with their constructors. The aim here is to extract the information we need from various APIs, and then use the constructors of these classes in order to create a homogenous array of objects that contains this information.\\\\
Sticking with the idea that we want our code to be generic and devoid of any service-specific details, we decided to include such details in external JSON files. When needed, these files will be parsed in order to identify how to query said services, and what parameters to include in the requests. The following figure shows a skeleton of a JSON service descriptor. The role of each object included in a JSON descriptor will be explained later on, but the point behind these descriptors is to know how to query the services, what parameters they expect, and how to parse their responses.
\begin{figure}[h]
\centering
\includegraphics[scale=0.49]{descriptor}
\caption{Descriptor Skeleton}
\end{figure}
\\\\\\
Since the requests will be received from the middleware, we have to provide a way for it to know what kind of resources are available, what are the corresponding request parameters it expects for each available API, as well as the structure of the response, so that the middleware can parse these responses. To that, the ListServices API was created, and the response of this API is a list of all of the available services, along with the relevant details. The following is a sample response, containing only two services (restaurants and events). Whenever new services are added, a corresponding description should be included in the following response.
\newpage
\begin{lstlisting}[language=json,firstnumber=1]
{
  "restaurants": {
    "endpoint": "GETRestaurants.php",
    "params": {
      "city": "string",
      "query": "string",
      "lon": "float",
      "lat": "float"
    },
    "attributes": {
      "name": "string",
      "address": "string",
      "url": "string",
      "latitude": "float",
      "longitude": "float",
      "geohash": "string",
      "cusisines": "string",
      "thumb": "string",
      "phone": "string",
      "rating": "float"
    }
  },
  "events": {
    ...
  },
  "news": {
    ...
  },
  "weather": {
    ...
  }
}
\end{lstlisting}
\pagebreak
\subsection{Storage and Caching}
After we developed a working system that can send requests to multiple services and integrate their responses, a testing phase took place in order to study the efficiency of this system and find ways to improve it. Right away, we noticed that querying multiple services and doing all of pre- and post-processing is a costly operation, especially in terms of running time. For instance, querying for music events that are taking place in Boston had an average running time of 30 seconds, during which we query four different events APIs and perform all of the necessary operations to integrate their responses. From a regular user's perspective, 30 seconds is quite a long wait duration for such a simple query, especially in the case where he wants to perform multiple searches with different keywords.\\\\
To mediate this drawback, we decided to add caching functionalities, thusly reducing the amount of requests we send out to the various web services. The caching will be done as shown in the following figure:\\
\begin{figure}[h]
\centering
\includegraphics[scale=0.46]{cachediagram}
\caption{Caching Schema}
\end{figure}
\\\\\\\\
The caching is done on two levels: query caching and response caching. The first caching level, query caching, consists of saving the information related to every query that is received from the middleware. Query caching allows us to check whether a query similar to one that we are about to send now has already been issued within a certain timeframe. In case the cache check results in a miss, that means that no similar query has been issued recently, and therefore we must query the web services to retrieve the requested information. However, it is worthy to note that as the system scales and the number of users increases, the number of instances where the query cache will result in a miss should decrease noticeably.\\\\
In case the query cache check results in a hit, we can safely assume that a similar query has been sent. This is where the second level of caching comes into play; the second caching level consists of saving the output of responses we receive when querying the web services. The information we store in the cache matches the structure of the resource schema, and this means that no further processing is needed whenever we query for this data later on.\\\\
The outcome of this process is a reduction in the number of requests we issue to the web services, which reduces the running time a lot since this is the actual bottleneck in our operation flow. Nonetheless, we still have to make sure that the cardinality of the response resulting from querying the second level of cache is above the required threshold. If that is not the case, we have to fall back again to querying the web services in order to update the resource cache with new data and refresh what already exists as well.